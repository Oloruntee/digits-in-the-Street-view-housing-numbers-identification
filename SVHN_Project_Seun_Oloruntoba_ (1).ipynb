{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft1e26NfbWdw"
   },
   "source": [
    "# **Background and Context**\n",
    "The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications. The Street View House Numbers (SVHN) dataset is one of the most popular ones. It has been used in neural networks created by Google to read house numbers and match them to their geolocations. This is a great benchmark dataset to play with, learn and train models that accurately identify street numbers, and incorporate them into all sorts of projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40NVrkQxWA_i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WGK82jAb0YI"
   },
   "source": [
    "# **Objective:**\n",
    "In this project, we will use a dataset with images centered around a single digit (many of the images do contain some distractors at the sides). Although we are taking a sample of the data which is simple. Given the dataset, our aim is to build a model that can identify house numbers in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcXp-9yfcNIz"
   },
   "source": [
    "# **Dataset**\n",
    "\n",
    "The dataset has the following features:\n",
    "\n",
    "Number of classes: 10\n",
    "\n",
    "Training data: 42000 images\n",
    "\n",
    "Testing data: 18000 images\n",
    "\n",
    "Note that we aim to use only 2000 samples from the train and the test data set for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DSPpymRWYxI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "95K3mpK_HGtf"
   },
   "outputs": [],
   "source": [
    "# importing the required packages  \n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYLHltPmHU1O",
    "outputId": "b69a1999-52d4-4c07-90c5-a6dcf4ac9763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#Mount the drive for google colab usage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UchicChyIkz-"
   },
   "outputs": [],
   "source": [
    "#defining the path of the dataset after uploadig the dataset.h5 into the drive\n",
    "dataset_file = '/content/drive/MyDrive/Project_SVHN/SVHN_single_grey1.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YV158SNL5OQ",
    "outputId": "8f34ca16-f73e-4570-e41a-837e85cbfa52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"SVHN_single_grey1.h5\" (mode r)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the .h5 file from the path\n",
    "import h5py\n",
    "h5f = h5py.File(dataset_file, 'r')\n",
    "h5f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aDal0KlrobZ",
    "outputId": "74395378-9f27-477d-d322-abdf9bafafc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test\n",
      "X_train\n",
      "X_val\n",
      "y_test\n",
      "y_train\n",
      "y_val\n"
     ]
    }
   ],
   "source": [
    "# Studying the number of keys in the file\n",
    "for key in h5f.keys():\n",
    "    print(key) #Names of the groups in HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_1LZOaUbjx5"
   },
   "source": [
    "***The dataset has 6 keys which are : X_test, X_train, X_val, y_test,y_train, y_val***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-t7To-So3g0",
    "outputId": "c14e2a51-f011-43f4-dcd1-24fca3e80b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.29239776e-260 1.11688278e-308 5.18067355e-318 ... 3.88209829e-265\n",
      " 3.88209829e-265 3.88209829e-265]\n"
     ]
    }
   ],
   "source": [
    "#Convert datafile to Numpy \n",
    "data = np.fromfile(dataset_file, dtype=float)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61WKwFy5voyu",
    "outputId": "3eed3e16-0e80-4a1a-a39b-f24f4573cd7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61455512,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm the Shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WlRuR5tkiT5X"
   },
   "outputs": [],
   "source": [
    "#Confirm the shape of each of the 6 keys \n",
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "X_val = h5f['X_val'][:]\n",
    "y_val = h5f['y_val'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hG24YeqivVH",
    "outputId": "78901de7-21b4-44ac-81fe-4a635593c45d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 32, 32)\n",
      "(42000, 32, 32)\n",
      "(60000, 32, 32)\n",
      "(18000,)\n",
      "(42000,)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "#Printing the X_test, y_test, X_train, y_train, X_val, y_val shapes \n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiV1qxRSdaag"
   },
   "source": [
    "**The image size **is** 32 x 32 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSBUEMbIeb7t"
   },
   "source": [
    "### **DATA VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "id": "WWAXA0CKcgSC",
    "outputId": "7c12750d-c90d-4ea2-ae16-7e66f239e15f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
     ]
    }
   ],
   "source": [
    "# visualizing the first 10 images in the X_train in the dataset and their labels\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label for each of the above image: %s' % (y_train[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EMS7K_letfh"
   },
   "source": [
    "## **Pre-Processing the Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N63mcf-eeKf1",
    "outputId": "e569b970-ecec-49d5-860e-386e8ae501b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 1024) (42000,)\n",
      "Validation set (60000, 1024) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the images\n",
    "image_vector_size = 32*32\n",
    "x_train = X_train.reshape(X_train.shape[0], image_vector_size)\n",
    "x_val = X_val.reshape(X_val.shape[0], image_vector_size)\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255.0\n",
    "x_val = x_val / 255.0\n",
    "\n",
    "print('Training set', x_train.shape, y_train.shape)\n",
    "print('Validation set', x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9ZYltMGeE6r"
   },
   "source": [
    "**The training and the validation data would be use for the model while the test data would be reserved for validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q4HIHV6Ekmch"
   },
   "outputs": [],
   "source": [
    "# Extract 2000 samples to be used in the modelling    \n",
    "x_tr =x_train[:2000]\n",
    "x_val =x_val[:2000]\n",
    "y_tr = y_train[:2000]\n",
    "y_val = y_val[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENAdSkq8LMbD",
    "outputId": "4e62ab4a-9f2d-4e60-9f93-56a7ab691650"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape # Confirm the 2000 sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "n5NhwJdS9WPa"
   },
   "outputs": [],
   "source": [
    "# Convert to the output variable to \"one-hot\" vectors using the to_categorical function\n",
    "num_classes = 10\n",
    "y_tr = to_categorical(y_tr,num_classes)\n",
    "y_val = to_categorical(y_val,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeCN6PvxMA8h",
    "outputId": "ad4e55a9-e571-4888-8988-6a4b39a5f40e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape #Confirm the 200 sample and the category number classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbHnDCt4kYcR"
   },
   "source": [
    "# **Defining the Sequential Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xb6WWqz8lVpj"
   },
   "outputs": [],
   "source": [
    "#Defining the Sequential Model\n",
    "image_size=32*32\n",
    "\n",
    "# create model\n",
    "model = Sequential()  \n",
    "model.add(Dense(256, activation='relu',kernel_initializer='he_uniform',input_shape=(image_size,))) ###Multiple Dense units with Relu activation\n",
    "model.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='Softmax')) ### For multiclass classification Softmax is used, this is num_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UoIXzQHNlV87"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#RMS_prop=optimizers.RMSprop()   ## we can similarly use different optimizers like RMSprop, Adagrad and SGD \n",
    "adam = optimizers.Adam(lr=1e-3) # Adam optimiser is used \n",
    "model.compile(loss= 'categorical_crossentropy', optimizer=adam, metrics=['accuracy']) ### Loss function used = Categorical cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_cPOZGBTSzW",
    "outputId": "69293dc3-277f-4a72-da2e-188cdcfba1d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 285,418\n",
      "Trainable params: 285,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Looking into our base model to get the summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oznwbA94T-TG",
    "outputId": "50aa6c76-2471-49b0-8f09-2c878443042e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 2s 43ms/step - loss: 2.3734 - accuracy: 0.0950 - val_loss: 2.6351 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 2.3166 - accuracy: 0.1030 - val_loss: 2.3512 - val_accuracy: 0.0010\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.3026 - accuracy: 0.1030 - val_loss: 2.4568 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.2996 - accuracy: 0.1050 - val_loss: 2.3225 - val_accuracy: 0.0035\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.3039 - accuracy: 0.1085 - val_loss: 2.0787 - val_accuracy: 0.9610\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 2.3046 - accuracy: 0.1050 - val_loss: 2.2026 - val_accuracy: 0.2730\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 2.2965 - accuracy: 0.1140 - val_loss: 2.4632 - val_accuracy: 0.0035\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 2.3008 - accuracy: 0.1170 - val_loss: 2.3973 - val_accuracy: 0.0120\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.3004 - accuracy: 0.1040 - val_loss: 2.3163 - val_accuracy: 0.0190\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3024 - accuracy: 0.1105 - val_loss: 2.1104 - val_accuracy: 0.3110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76e8481190>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr, y_tr, validation_data=(x_val, y_val), batch_size=128, epochs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47AD_S2No_0e"
   },
   "source": [
    "Observation : \n",
    "*   The accuracy for both Training and Validation at Epoch 10 are very low \n",
    "*   The losses are very high at Epoch 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv3FkAHimKzL"
   },
   "source": [
    "## **Tune the Hyperparameters of the model**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciM1YwKJKj8m",
    "outputId": "66bf476d-7452-42fa-b247-7d3a24a3d86f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.2997 - accuracy: 0.1190 - val_loss: 2.1526 - val_accuracy: 0.6070\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2994 - accuracy: 0.1215 - val_loss: 2.3562 - val_accuracy: 0.0165\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2951 - accuracy: 0.1210 - val_loss: 2.1953 - val_accuracy: 0.5120\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.2884 - accuracy: 0.1360 - val_loss: 2.2434 - val_accuracy: 0.1995\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.2763 - accuracy: 0.1510 - val_loss: 2.2004 - val_accuracy: 0.5760\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.2603 - accuracy: 0.1625 - val_loss: 2.2097 - val_accuracy: 0.3700\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2414 - accuracy: 0.1790 - val_loss: 2.4175 - val_accuracy: 0.0385\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.2294 - accuracy: 0.1835 - val_loss: 2.0035 - val_accuracy: 0.7550\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.1751 - accuracy: 0.2045 - val_loss: 2.2008 - val_accuracy: 0.3055\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.1278 - accuracy: 0.2410 - val_loss: 2.1455 - val_accuracy: 0.3400\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.0678 - accuracy: 0.2435 - val_loss: 2.0528 - val_accuracy: 0.4005\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.0257 - accuracy: 0.2610 - val_loss: 2.0975 - val_accuracy: 0.3100\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.9493 - accuracy: 0.2775 - val_loss: 2.2617 - val_accuracy: 0.0785\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.8930 - accuracy: 0.3195 - val_loss: 2.2018 - val_accuracy: 0.1680\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.8575 - accuracy: 0.3195 - val_loss: 1.9423 - val_accuracy: 0.3440\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.8395 - accuracy: 0.3285 - val_loss: 2.0997 - val_accuracy: 0.3555\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7689 - accuracy: 0.3725 - val_loss: 2.0137 - val_accuracy: 0.3335\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7237 - accuracy: 0.3920 - val_loss: 2.0499 - val_accuracy: 0.1065\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7243 - accuracy: 0.3985 - val_loss: 2.0898 - val_accuracy: 0.2755\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.6442 - accuracy: 0.4290 - val_loss: 1.7794 - val_accuracy: 0.3810\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.6163 - accuracy: 0.4335 - val_loss: 1.8462 - val_accuracy: 0.3670\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5931 - accuracy: 0.4440 - val_loss: 1.8997 - val_accuracy: 0.3805\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.5680 - accuracy: 0.4565 - val_loss: 1.8491 - val_accuracy: 0.3915\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5519 - accuracy: 0.4650 - val_loss: 1.7349 - val_accuracy: 0.4750\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.5063 - accuracy: 0.4835 - val_loss: 1.8210 - val_accuracy: 0.4310\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4568 - accuracy: 0.4955 - val_loss: 1.8121 - val_accuracy: 0.3895\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.4342 - accuracy: 0.5020 - val_loss: 1.7440 - val_accuracy: 0.4825\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4068 - accuracy: 0.5120 - val_loss: 1.4883 - val_accuracy: 0.5865\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.3948 - accuracy: 0.5240 - val_loss: 1.6415 - val_accuracy: 0.5285\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4230 - accuracy: 0.5095 - val_loss: 1.7719 - val_accuracy: 0.4625\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3759 - accuracy: 0.5350 - val_loss: 1.6765 - val_accuracy: 0.4395\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3787 - accuracy: 0.5115 - val_loss: 1.6867 - val_accuracy: 0.4735\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3703 - accuracy: 0.5225 - val_loss: 1.6824 - val_accuracy: 0.4675\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.3432 - accuracy: 0.5355 - val_loss: 1.5145 - val_accuracy: 0.5735\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.3637 - accuracy: 0.5255 - val_loss: 1.3624 - val_accuracy: 0.6020\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3206 - accuracy: 0.5465 - val_loss: 1.7288 - val_accuracy: 0.4930\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2647 - accuracy: 0.5865 - val_loss: 1.7396 - val_accuracy: 0.4660\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2624 - accuracy: 0.5610 - val_loss: 1.3692 - val_accuracy: 0.6170\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.2548 - accuracy: 0.5765 - val_loss: 1.4213 - val_accuracy: 0.5635\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.2269 - accuracy: 0.5880 - val_loss: 1.3022 - val_accuracy: 0.6570\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2272 - accuracy: 0.5840 - val_loss: 1.3245 - val_accuracy: 0.6550\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1535 - accuracy: 0.6105 - val_loss: 1.4833 - val_accuracy: 0.5495\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.1521 - accuracy: 0.6120 - val_loss: 1.2988 - val_accuracy: 0.6105\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1361 - accuracy: 0.6165 - val_loss: 1.4029 - val_accuracy: 0.5970\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1445 - accuracy: 0.6180 - val_loss: 1.2586 - val_accuracy: 0.6700\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1326 - accuracy: 0.6240 - val_loss: 1.5281 - val_accuracy: 0.5650\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.1576 - accuracy: 0.6150 - val_loss: 1.4129 - val_accuracy: 0.6125\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1064 - accuracy: 0.6180 - val_loss: 1.1216 - val_accuracy: 0.7215\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0834 - accuracy: 0.6460 - val_loss: 1.2393 - val_accuracy: 0.6775\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1011 - accuracy: 0.6320 - val_loss: 1.5779 - val_accuracy: 0.5630\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0701 - accuracy: 0.6475 - val_loss: 1.1380 - val_accuracy: 0.7040\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0588 - accuracy: 0.6440 - val_loss: 1.4848 - val_accuracy: 0.5565\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0708 - accuracy: 0.6515 - val_loss: 1.0240 - val_accuracy: 0.7370\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0585 - accuracy: 0.6440 - val_loss: 1.5117 - val_accuracy: 0.5685\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0611 - accuracy: 0.6480 - val_loss: 1.4346 - val_accuracy: 0.5960\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0393 - accuracy: 0.6655 - val_loss: 1.2681 - val_accuracy: 0.6350\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0188 - accuracy: 0.6630 - val_loss: 1.3474 - val_accuracy: 0.6300\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0261 - accuracy: 0.6680 - val_loss: 1.2340 - val_accuracy: 0.6750\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0311 - accuracy: 0.6555 - val_loss: 1.0924 - val_accuracy: 0.7240\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9749 - accuracy: 0.6735 - val_loss: 1.4991 - val_accuracy: 0.5830\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9505 - accuracy: 0.6915 - val_loss: 1.2463 - val_accuracy: 0.6650\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9369 - accuracy: 0.6900 - val_loss: 1.5612 - val_accuracy: 0.5335\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0033 - accuracy: 0.6600 - val_loss: 1.2646 - val_accuracy: 0.6430\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9622 - accuracy: 0.6850 - val_loss: 1.4011 - val_accuracy: 0.6255\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9509 - accuracy: 0.6815 - val_loss: 1.2742 - val_accuracy: 0.6605\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9423 - accuracy: 0.6815 - val_loss: 1.3260 - val_accuracy: 0.6240\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9842 - accuracy: 0.6690 - val_loss: 1.4307 - val_accuracy: 0.5855\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9435 - accuracy: 0.6835 - val_loss: 1.3499 - val_accuracy: 0.5820\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9200 - accuracy: 0.6975 - val_loss: 1.2713 - val_accuracy: 0.6355\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9051 - accuracy: 0.7060 - val_loss: 1.2785 - val_accuracy: 0.6450\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8877 - accuracy: 0.7110 - val_loss: 1.2110 - val_accuracy: 0.6525\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9484 - accuracy: 0.6790 - val_loss: 1.5463 - val_accuracy: 0.5645\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8955 - accuracy: 0.7020 - val_loss: 1.2540 - val_accuracy: 0.6555\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8465 - accuracy: 0.7155 - val_loss: 1.3958 - val_accuracy: 0.6100\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8417 - accuracy: 0.7230 - val_loss: 1.3143 - val_accuracy: 0.6265\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8096 - accuracy: 0.7270 - val_loss: 1.3880 - val_accuracy: 0.6355\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8273 - accuracy: 0.7310 - val_loss: 1.2845 - val_accuracy: 0.6525\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8167 - accuracy: 0.7335 - val_loss: 1.0284 - val_accuracy: 0.7270\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8364 - accuracy: 0.7275 - val_loss: 1.4027 - val_accuracy: 0.6020\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7920 - accuracy: 0.7330 - val_loss: 1.4888 - val_accuracy: 0.6015\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7797 - accuracy: 0.7430 - val_loss: 1.2450 - val_accuracy: 0.6645\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8186 - accuracy: 0.7225 - val_loss: 1.2492 - val_accuracy: 0.6725\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8296 - accuracy: 0.7265 - val_loss: 1.4044 - val_accuracy: 0.6025\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7784 - accuracy: 0.7395 - val_loss: 1.6304 - val_accuracy: 0.5185\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8081 - accuracy: 0.7305 - val_loss: 1.6117 - val_accuracy: 0.5400\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8364 - accuracy: 0.7210 - val_loss: 1.2666 - val_accuracy: 0.6420\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7561 - accuracy: 0.7555 - val_loss: 1.1976 - val_accuracy: 0.6570\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7449 - accuracy: 0.7560 - val_loss: 1.2361 - val_accuracy: 0.6795\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7267 - accuracy: 0.7575 - val_loss: 1.3008 - val_accuracy: 0.6630\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7166 - accuracy: 0.7665 - val_loss: 1.3125 - val_accuracy: 0.6365\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7000 - accuracy: 0.7655 - val_loss: 1.2800 - val_accuracy: 0.6590\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7102 - accuracy: 0.7730 - val_loss: 1.2508 - val_accuracy: 0.6645\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6924 - accuracy: 0.7705 - val_loss: 1.1567 - val_accuracy: 0.7080\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7083 - accuracy: 0.7620 - val_loss: 1.4305 - val_accuracy: 0.6335\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6977 - accuracy: 0.7675 - val_loss: 1.3829 - val_accuracy: 0.6285\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7489 - accuracy: 0.7480 - val_loss: 1.4106 - val_accuracy: 0.6355\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7343 - accuracy: 0.7490 - val_loss: 1.1567 - val_accuracy: 0.6980\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6864 - accuracy: 0.7730 - val_loss: 1.4295 - val_accuracy: 0.6160\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6761 - accuracy: 0.7770 - val_loss: 1.5785 - val_accuracy: 0.5740\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6631 - accuracy: 0.7955 - val_loss: 1.2248 - val_accuracy: 0.6715\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6394 - accuracy: 0.7925 - val_loss: 1.1507 - val_accuracy: 0.7025\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6438 - accuracy: 0.7880 - val_loss: 1.3487 - val_accuracy: 0.6410\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6745 - accuracy: 0.7805 - val_loss: 1.3964 - val_accuracy: 0.6250\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6387 - accuracy: 0.7935 - val_loss: 1.4330 - val_accuracy: 0.6240\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6484 - accuracy: 0.7915 - val_loss: 1.1619 - val_accuracy: 0.6985\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6272 - accuracy: 0.7925 - val_loss: 1.3729 - val_accuracy: 0.6320\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6607 - accuracy: 0.7790 - val_loss: 1.3195 - val_accuracy: 0.6610\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6061 - accuracy: 0.8070 - val_loss: 1.3717 - val_accuracy: 0.6385\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6132 - accuracy: 0.8040 - val_loss: 1.5726 - val_accuracy: 0.6005\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5765 - accuracy: 0.8120 - val_loss: 1.3155 - val_accuracy: 0.6585\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5530 - accuracy: 0.8290 - val_loss: 1.3116 - val_accuracy: 0.6530\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5686 - accuracy: 0.8195 - val_loss: 1.3571 - val_accuracy: 0.6545\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6065 - accuracy: 0.7970 - val_loss: 1.9469 - val_accuracy: 0.5155\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6062 - accuracy: 0.8020 - val_loss: 1.2282 - val_accuracy: 0.6850\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5789 - accuracy: 0.8100 - val_loss: 1.2566 - val_accuracy: 0.6750\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5602 - accuracy: 0.8105 - val_loss: 1.2738 - val_accuracy: 0.6775\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5695 - accuracy: 0.8105 - val_loss: 1.2565 - val_accuracy: 0.6875\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5686 - accuracy: 0.8210 - val_loss: 1.3965 - val_accuracy: 0.6370\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5313 - accuracy: 0.8345 - val_loss: 1.5351 - val_accuracy: 0.6080\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5488 - accuracy: 0.8175 - val_loss: 1.3918 - val_accuracy: 0.6455\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5057 - accuracy: 0.8390 - val_loss: 1.5066 - val_accuracy: 0.6210\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5251 - accuracy: 0.8280 - val_loss: 1.5081 - val_accuracy: 0.6175\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5285 - accuracy: 0.8345 - val_loss: 1.3569 - val_accuracy: 0.6585\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5406 - accuracy: 0.8255 - val_loss: 1.0894 - val_accuracy: 0.7325\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5341 - accuracy: 0.8355 - val_loss: 1.5713 - val_accuracy: 0.6155\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5606 - accuracy: 0.8265 - val_loss: 1.0038 - val_accuracy: 0.7465\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6089 - accuracy: 0.7895 - val_loss: 1.6232 - val_accuracy: 0.5925\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5377 - accuracy: 0.8195 - val_loss: 1.4208 - val_accuracy: 0.6315\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5213 - accuracy: 0.8350 - val_loss: 1.1314 - val_accuracy: 0.7240\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5404 - accuracy: 0.8185 - val_loss: 1.6261 - val_accuracy: 0.6005\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5552 - accuracy: 0.8145 - val_loss: 1.4332 - val_accuracy: 0.6400\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5218 - accuracy: 0.8340 - val_loss: 1.4334 - val_accuracy: 0.6325\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5253 - accuracy: 0.8285 - val_loss: 1.3104 - val_accuracy: 0.6830\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5428 - accuracy: 0.8225 - val_loss: 1.6874 - val_accuracy: 0.5745\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5016 - accuracy: 0.8295 - val_loss: 1.5160 - val_accuracy: 0.6075\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5010 - accuracy: 0.8340 - val_loss: 1.4379 - val_accuracy: 0.6460\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4679 - accuracy: 0.8535 - val_loss: 1.5154 - val_accuracy: 0.6310\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4588 - accuracy: 0.8505 - val_loss: 1.3892 - val_accuracy: 0.6605\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4459 - accuracy: 0.8645 - val_loss: 1.2971 - val_accuracy: 0.6850\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4547 - accuracy: 0.8550 - val_loss: 1.4299 - val_accuracy: 0.6510\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4783 - accuracy: 0.8460 - val_loss: 1.5755 - val_accuracy: 0.6235\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4545 - accuracy: 0.8560 - val_loss: 1.5662 - val_accuracy: 0.6325\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4345 - accuracy: 0.8570 - val_loss: 1.4185 - val_accuracy: 0.6610\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4232 - accuracy: 0.8655 - val_loss: 1.4817 - val_accuracy: 0.6410\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4615 - accuracy: 0.8485 - val_loss: 1.4962 - val_accuracy: 0.6445\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4096 - accuracy: 0.8700 - val_loss: 1.7781 - val_accuracy: 0.5825\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4138 - accuracy: 0.8695 - val_loss: 1.5102 - val_accuracy: 0.6495\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4164 - accuracy: 0.8705 - val_loss: 1.7031 - val_accuracy: 0.6145\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4211 - accuracy: 0.8705 - val_loss: 1.4807 - val_accuracy: 0.6560\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4513 - accuracy: 0.8525 - val_loss: 1.6396 - val_accuracy: 0.6290\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4168 - accuracy: 0.8675 - val_loss: 1.4837 - val_accuracy: 0.6665\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4492 - accuracy: 0.8575 - val_loss: 1.2268 - val_accuracy: 0.7140\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4199 - accuracy: 0.8655 - val_loss: 1.6948 - val_accuracy: 0.6105\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4113 - accuracy: 0.8685 - val_loss: 1.7255 - val_accuracy: 0.6080\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3939 - accuracy: 0.8745 - val_loss: 1.4519 - val_accuracy: 0.6710\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3824 - accuracy: 0.8740 - val_loss: 1.7775 - val_accuracy: 0.5925\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3623 - accuracy: 0.8875 - val_loss: 1.6612 - val_accuracy: 0.6170\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3450 - accuracy: 0.8940 - val_loss: 1.7800 - val_accuracy: 0.6185\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3541 - accuracy: 0.8930 - val_loss: 1.5800 - val_accuracy: 0.6480\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3573 - accuracy: 0.8890 - val_loss: 1.7476 - val_accuracy: 0.6130\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3480 - accuracy: 0.8930 - val_loss: 1.7173 - val_accuracy: 0.6340\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3604 - accuracy: 0.8800 - val_loss: 2.0352 - val_accuracy: 0.5785\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3845 - accuracy: 0.8750 - val_loss: 1.7349 - val_accuracy: 0.6135\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4164 - accuracy: 0.8650 - val_loss: 1.5327 - val_accuracy: 0.6625\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3658 - accuracy: 0.8830 - val_loss: 1.5752 - val_accuracy: 0.6600\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3358 - accuracy: 0.8955 - val_loss: 1.7025 - val_accuracy: 0.6225\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3257 - accuracy: 0.9005 - val_loss: 1.9517 - val_accuracy: 0.5900\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3453 - accuracy: 0.8925 - val_loss: 1.9971 - val_accuracy: 0.5575\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3601 - accuracy: 0.8855 - val_loss: 1.5990 - val_accuracy: 0.6355\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3372 - accuracy: 0.8970 - val_loss: 1.9043 - val_accuracy: 0.5905\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3490 - accuracy: 0.8805 - val_loss: 1.5980 - val_accuracy: 0.6420\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3311 - accuracy: 0.8990 - val_loss: 1.7067 - val_accuracy: 0.6255\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3488 - accuracy: 0.8840 - val_loss: 1.6758 - val_accuracy: 0.6545\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3524 - accuracy: 0.8840 - val_loss: 1.6020 - val_accuracy: 0.6625\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3625 - accuracy: 0.8825 - val_loss: 1.6428 - val_accuracy: 0.6490\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3279 - accuracy: 0.8880 - val_loss: 1.7337 - val_accuracy: 0.6425\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3495 - accuracy: 0.8785 - val_loss: 1.5666 - val_accuracy: 0.6795\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2919 - accuracy: 0.9120 - val_loss: 1.5208 - val_accuracy: 0.6710\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2840 - accuracy: 0.9170 - val_loss: 1.9063 - val_accuracy: 0.6050\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3006 - accuracy: 0.9095 - val_loss: 1.5378 - val_accuracy: 0.6795\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3132 - accuracy: 0.9035 - val_loss: 1.9345 - val_accuracy: 0.6055\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3261 - accuracy: 0.8915 - val_loss: 2.0460 - val_accuracy: 0.5945\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2993 - accuracy: 0.9065 - val_loss: 1.8499 - val_accuracy: 0.6275\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2835 - accuracy: 0.9190 - val_loss: 1.5923 - val_accuracy: 0.6740\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2884 - accuracy: 0.9145 - val_loss: 1.9134 - val_accuracy: 0.6190\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2985 - accuracy: 0.9010 - val_loss: 1.4636 - val_accuracy: 0.6920\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2618 - accuracy: 0.9225 - val_loss: 1.7015 - val_accuracy: 0.6480\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2519 - accuracy: 0.9195 - val_loss: 1.6549 - val_accuracy: 0.6780\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2421 - accuracy: 0.9345 - val_loss: 1.8461 - val_accuracy: 0.6305\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2395 - accuracy: 0.9255 - val_loss: 1.8515 - val_accuracy: 0.6380\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2243 - accuracy: 0.9355 - val_loss: 2.2317 - val_accuracy: 0.5870\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3581 - accuracy: 0.8830 - val_loss: 2.2363 - val_accuracy: 0.5640\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3660 - accuracy: 0.8730 - val_loss: 2.0237 - val_accuracy: 0.5925\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3003 - accuracy: 0.9020 - val_loss: 1.9198 - val_accuracy: 0.6235\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5206 - accuracy: 0.8350 - val_loss: 1.9709 - val_accuracy: 0.5840\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4841 - accuracy: 0.8375 - val_loss: 1.7032 - val_accuracy: 0.6515\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3346 - accuracy: 0.8965 - val_loss: 1.9130 - val_accuracy: 0.6190\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2522 - accuracy: 0.9265 - val_loss: 1.7062 - val_accuracy: 0.6550\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2390 - accuracy: 0.9305 - val_loss: 2.0216 - val_accuracy: 0.6100\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2428 - accuracy: 0.9305 - val_loss: 1.9669 - val_accuracy: 0.6245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76ecb84450>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increase the Epoch to 200\n",
    "model.fit(x_tr, y_tr, validation_data=(x_val, y_val), batch_size=128, epochs=200)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Kfqc8ykqfLJ"
   },
   "source": [
    "# Observation  \n",
    "The accuracy increased for the training dataset from 0.377 to 0.933 and the loss reduces as well.\n",
    "\n",
    "The accuracy increased for the validation dataset from 0.480 to 0.629 and the loss reduces as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UDWsfzFqr6O",
    "outputId": "339ae1dd-1634-492f-c257-b657a3e470ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 1s 22ms/step - loss: 2.3618 - accuracy: 0.1020 - val_loss: 2.3203 - val_accuracy: 0.0030\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3233 - accuracy: 0.1130 - val_loss: 2.4520 - val_accuracy: 5.0000e-04\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3118 - accuracy: 0.1160 - val_loss: 2.3052 - val_accuracy: 0.0075\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3065 - accuracy: 0.1175 - val_loss: 2.3875 - val_accuracy: 0.0025\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3138 - accuracy: 0.1015 - val_loss: 2.4318 - val_accuracy: 0.0015\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3078 - accuracy: 0.1220 - val_loss: 2.2384 - val_accuracy: 0.2040\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3074 - accuracy: 0.1030 - val_loss: 2.3707 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3000 - accuracy: 0.1130 - val_loss: 2.3817 - val_accuracy: 0.0030\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2979 - accuracy: 0.1145 - val_loss: 2.3449 - val_accuracy: 0.0325\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2852 - accuracy: 0.1440 - val_loss: 2.4193 - val_accuracy: 5.0000e-04\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2674 - accuracy: 0.1555 - val_loss: 2.2941 - val_accuracy: 0.1680\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2592 - accuracy: 0.1650 - val_loss: 2.2560 - val_accuracy: 0.0800\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2320 - accuracy: 0.1895 - val_loss: 2.0293 - val_accuracy: 0.6405\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2.1820 - accuracy: 0.1965 - val_loss: 2.2419 - val_accuracy: 0.0275\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.1440 - accuracy: 0.2170 - val_loss: 2.2440 - val_accuracy: 0.2890\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.0796 - accuracy: 0.2460 - val_loss: 2.1969 - val_accuracy: 0.2565\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.9856 - accuracy: 0.3035 - val_loss: 1.8607 - val_accuracy: 0.4635\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.8663 - accuracy: 0.3505 - val_loss: 1.8583 - val_accuracy: 0.5170\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7920 - accuracy: 0.3620 - val_loss: 1.6661 - val_accuracy: 0.5735\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7508 - accuracy: 0.3885 - val_loss: 2.2538 - val_accuracy: 0.2480\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.6268 - accuracy: 0.4425 - val_loss: 1.8705 - val_accuracy: 0.4315\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.6208 - accuracy: 0.4410 - val_loss: 2.1545 - val_accuracy: 0.2145\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5664 - accuracy: 0.4570 - val_loss: 2.0860 - val_accuracy: 0.3190\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4992 - accuracy: 0.5040 - val_loss: 1.4639 - val_accuracy: 0.6120\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4270 - accuracy: 0.5075 - val_loss: 1.1865 - val_accuracy: 0.7170\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5138 - accuracy: 0.4925 - val_loss: 1.3659 - val_accuracy: 0.6395\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4291 - accuracy: 0.5040 - val_loss: 1.4137 - val_accuracy: 0.6250\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2988 - accuracy: 0.5655 - val_loss: 1.8635 - val_accuracy: 0.4400\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2743 - accuracy: 0.5810 - val_loss: 1.3266 - val_accuracy: 0.6460\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2591 - accuracy: 0.5650 - val_loss: 1.6127 - val_accuracy: 0.5320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76ecb3c590>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the number of Neurons \n",
    "\n",
    "#Defining the Sequential Model\n",
    "image_size=32*32\n",
    "\n",
    "# create model\n",
    "model = Sequential()  \n",
    "model.add(Dense(256, activation='relu',kernel_initializer='he_uniform',input_shape=(image_size,))) ###Multiple Dense units with Relu activation\n",
    "model.add(Dense(128, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(128, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='Softmax')) ### For multiclass classification Softmax is used, this is num_class = 10\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr, y_tr, validation_data=(x_val, y_val), batch_size=128, epochs=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0y24m-Dj_tEy",
    "outputId": "5706ba47-b27f-4dda-9cdf-de68fa95a67b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 1s 20ms/step - loss: 2.4324 - accuracy: 0.1020 - val_loss: 2.4740 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3353 - accuracy: 0.1135 - val_loss: 2.2879 - val_accuracy: 0.0065\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.3119 - accuracy: 0.1195 - val_loss: 2.3585 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3234 - accuracy: 0.1070 - val_loss: 2.5008 - val_accuracy: 5.0000e-04\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3105 - accuracy: 0.1090 - val_loss: 2.9605 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3019 - accuracy: 0.1320 - val_loss: 2.5320 - val_accuracy: 0.0010\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2898 - accuracy: 0.1345 - val_loss: 2.3701 - val_accuracy: 0.0125\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2847 - accuracy: 0.1420 - val_loss: 2.2964 - val_accuracy: 0.0345\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2586 - accuracy: 0.1745 - val_loss: 2.1164 - val_accuracy: 0.3515\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2410 - accuracy: 0.1670 - val_loss: 2.0864 - val_accuracy: 0.1850\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2245 - accuracy: 0.1990 - val_loss: 2.4364 - val_accuracy: 0.0310\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.1652 - accuracy: 0.2140 - val_loss: 2.7433 - val_accuracy: 0.0155\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.0750 - accuracy: 0.2570 - val_loss: 1.9233 - val_accuracy: 0.4430\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.9762 - accuracy: 0.3025 - val_loss: 2.6186 - val_accuracy: 0.1155\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.9388 - accuracy: 0.2960 - val_loss: 2.0679 - val_accuracy: 0.3755\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.8156 - accuracy: 0.3630 - val_loss: 1.7921 - val_accuracy: 0.4585\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.8250 - accuracy: 0.3450 - val_loss: 1.9173 - val_accuracy: 0.3845\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.6367 - accuracy: 0.4365 - val_loss: 1.6277 - val_accuracy: 0.5420\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5978 - accuracy: 0.4280 - val_loss: 2.2956 - val_accuracy: 0.1980\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5214 - accuracy: 0.4775 - val_loss: 1.3826 - val_accuracy: 0.5460\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.5323 - accuracy: 0.4715 - val_loss: 1.6740 - val_accuracy: 0.4290\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4166 - accuracy: 0.5260 - val_loss: 1.4755 - val_accuracy: 0.5520\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.3624 - accuracy: 0.5425 - val_loss: 1.9023 - val_accuracy: 0.2885\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3818 - accuracy: 0.5295 - val_loss: 1.4577 - val_accuracy: 0.5475\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2905 - accuracy: 0.5660 - val_loss: 2.3871 - val_accuracy: 0.2535\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3106 - accuracy: 0.5720 - val_loss: 1.3953 - val_accuracy: 0.5405\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2929 - accuracy: 0.5685 - val_loss: 1.9381 - val_accuracy: 0.3310\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2112 - accuracy: 0.5970 - val_loss: 1.6457 - val_accuracy: 0.4320\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1518 - accuracy: 0.6260 - val_loss: 1.8926 - val_accuracy: 0.3635\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1770 - accuracy: 0.6045 - val_loss: 1.0693 - val_accuracy: 0.7050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76e2d08a90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the activation function from relu to elu for the last two neurons\n",
    "\n",
    "#Defining the Sequential Model\n",
    "image_size=32*32\n",
    "\n",
    "# create model\n",
    "model = Sequential()  \n",
    "model.add(Dense(256, activation='relu',kernel_initializer='he_uniform',input_shape=(image_size,))) ###Multiple Dense units with Relu activation\n",
    "model.add(Dense(128, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(128, activation='elu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(64, activation='elu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='Softmax')) ### For multiclass classification Softmax is used, this is num_class = 10\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr, y_tr, validation_data=(x_val, y_val), batch_size=128, epochs=30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsuiHoLHsyzA"
   },
   "source": [
    "Observation:\n",
    "22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nuSdZgG8uAYy"
   },
   "outputs": [],
   "source": [
    "# Testing the model (Pre-processing)\n",
    "x_test = X_test.reshape(X_test.shape[0], image_vector_size)\n",
    "x_test = x_test / 255.0\n",
    "y_test = to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va5tE3E8uU7u",
    "outputId": "10ea28a2-49a7-4121-a305-6547d5168510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 1.4714 - accuracy: 0.5234\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AH-ob6Ziy-4m",
    "outputId": "96946351-b8b2-4c58-debe-d0f041512b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 1.4714 - accuracy: 0.5234\n",
      "Test loss: 1.471402645111084\n",
      "Test accuracy: 0.5233888626098633\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w0xzjqVzVAG"
   },
   "source": [
    "#Summary and Observation \n",
    "\n",
    "1. The image format required the .h5 file to be read with a h5py file format \n",
    "\n",
    "2. The data had to be converted into an array before it can be used for the modelling \n",
    "\n",
    "3. The data were given for train, validation and test, while the train and validation were used in the modelling the test was only used after the model has been completed to avoid data leakage \n",
    "\n",
    "4. Also, noticed the difference in the accuracy and loss based on the different Neural Network Hyperparameter tunning such as: Number of neurons, activation function, optimizer, learning rate, batch size, or epochs, even though not all were tunned in the project\n",
    "\n",
    "5. The test accuracy was a bit less than the train result but higher than the validation result, with this we could say it does not overfit even though the result was not too good. \n",
    "\n",
    "6. Increase in epoch tend to result in the increase of the accuracy of the  training data but the validation hover around 0.5 - 0.6 irrespective of the number of epoch which means the model overfits when the epoch is very high. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Q13GfFaAzSXA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SVHN_Project_Seun Oloruntoba.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
